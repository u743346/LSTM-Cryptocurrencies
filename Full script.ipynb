{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full script for LSTM cryptocurrency prediction\n",
    "\n",
    "u743346\n",
    "<br>\n",
    "Gerlof Bremmer\n",
    "<br>\n",
    "g.d.bremmer@tilburguniversity.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import neccasary packages and set random seed for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import time\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from pandas import Series\n",
    "from pandas import Panel\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras import regularizers\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 1337\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pylab inline to show graphs inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gerlofbremmer/anaconda/lib/python3.5/site-packages/IPython/core/magics/pylab.py:161: UserWarning: pylab import has clobbered these variables: ['seed', 'sqrt']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Comment data pre-processing and sentiment calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load scraped data json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ltc = pd.read_json('ltc.json', lines=True)\n",
    "xrp = pd.read_json('xrp.json', lines=True)\n",
    "eth = pd.read_json('eth.json', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ltc.info()\n",
    "eth.info()\n",
    "xrp.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test if all comment ID's are unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique(df):\n",
    "    return df.id.nunique()\n",
    "\n",
    "print(unique(ltc))\n",
    "print(unique(eth))\n",
    "print(unique(xrp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filtering the dataset to only usefull columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_col(df, cols):\n",
    "    return df.drop(df.columns[[cols]], axis=1)\n",
    "\n",
    "ltc = filter_col(ltc, [0,2,3,4,5,6,7,8,9,11,12,13,14,15,16,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,35,36,37,38,39])\n",
    "eth = filter_col(eth, [0,2,3,4,5,6,7,8,9,11,12,13,14,15,16,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,35,36,37,38,39])\n",
    "xrp = filter_col(xrp, [0,2,3,4,5,6,7,8,9,11,12,13,14,15,16,18,19,20,22,23,24,25,26,27,28,29,30,31,32,33,35,36,37,38,39])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop comments from before 01-01-2016 = after epoch 1451606401 & before 1514764799"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ltc = ltc[ltc['created_utc'] >= 1451606401]\n",
    "ltc = ltc[ltc['created_utc'] <= 1514764799]\n",
    "eth = eth[eth['created_utc'] >= 1451606401]\n",
    "eth = eth[eth['created_utc'] <= 1514764799]\n",
    "xrp = xrp[xrp['created_utc'] >= 1451606401]\n",
    "xrp = xrp[xrp['created_utc'] <= 1514764799]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change created_utc to readble datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_datetime(df):\n",
    "    return pd.to_datetime(df['created_utc'],unit='s')\n",
    "\n",
    "ltc['datetime'] = to_datetime(ltc)\n",
    "eth['datetime'] = to_datetime(eth)\n",
    "xrp['datetime'] = to_datetime(xrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove 'removed' and 'deleted' comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ltc = ltc[ltc.body != '[removed]']\n",
    "ltc = ltc[ltc.body != '[deleted]']\n",
    "xrp = xrp[xrp.body != '[removed]']\n",
    "xrp = xrp[xrp.body != '[deleted]']\n",
    "eth = eth[eth.body != '[removed]']\n",
    "eth = eth[eth.body != '[deleted]']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform VADER sentiment analysis on every sentence; new columns: compound, negative, neutral, positive\n",
    "<br>\n",
    "NOTE: computational extensive process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "ltc[['compound','neg','neu','pos']] = ltc['body'].apply(lambda body: pd.Series(analyzer.polarity_scores(body)))\n",
    "xrp[['compound','neg','neu','pos']] = xrp['body'].apply(lambda body: pd.Series(analyzer.polarity_scores(body)))\n",
    "eth[['compound','neg','neu','pos']] = eth['body'].apply(lambda body: pd.Series(analyzer.polarity_scores(body)))\n",
    "                                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grouping the data on datetime (day) simoultaniously calculating mean of sentiment scores + drop nan values\n",
    "\n",
    "resulting in 730 entries for LTC and ETH and 538 days for XRP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def mean_grouping(df):\n",
    "    return df.set_index('datetime').groupby(pd.TimeGrouper('D')).mean().dropna()\n",
    "\n",
    "ltccomment = mean_grouping(ltc)\n",
    "xrpcomment = mean_grouping(xrp)\n",
    "ethcomment = mean_grouping(eth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting number of comments per day and adding column numcomments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def comment_count(df):\n",
    "    return df.set_index('datetime').resample('D').size()\n",
    "    \n",
    "ltccomment['numcomments'] = comment_count(ltc)\n",
    "xrpcomment['numcomments'] = comment_count(xrp)\n",
    "ethcomment['numcomments'] = comment_count(eth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean, max, min for sentiment compound and karma score of comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def min_max_mean(coin, df):\n",
    "    print(\"Minumum compound \", coin,\" :\" ,df['compound'].min())\n",
    "    print(\"Maximum compound \", coin,\" :\" ,df['compound'].max())\n",
    "    print(\"Mean of compound \", coin,\" :\" ,df['compound'].mean())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "min_max_mean('LTC', ltc)\n",
    "min_max_mean('ETH', eth)\n",
    "min_max_mean('XRP', xrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def min_max_score(coin, df):\n",
    "    print(\"Minumum score\",coin,\" : \" ,df['score'].min())\n",
    "    print(\"Maximum score\",coin,\" : \" ,df['score'].max())\n",
    "    print(\"\\n\")\n",
    "    \n",
    "min_max_score('LTC', ltc)\n",
    "min_max_score('ETH', eth)\n",
    "min_max_score('XRP', xrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving DF with sentiment as pickle for later analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ltccomment.to_pickle('Litecoin2016-17.pkl')\n",
    "ethcomment.to_pickle('Ethereum2016-17.pkl')\n",
    "xrpcomment.to_pickle('Ripple2016-17.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Price data pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in prices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prices = pd.read_csv(\"prices.csv\" , parse_dates= ['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prices.info()\n",
    "prices[0:100]\n",
    "prices.loc[prices['symbol'] == 'XRP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 3 datasets with date as index (corrosponding to sentiment data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_index(df, coin):\n",
    "    return df[df['symbol'] == coin].set_index('date')\n",
    "\n",
    "pricesLTC = date_index(prices, 'LTC')\n",
    "pricesETH = date_index(prices, 'ETH')\n",
    "pricesXRP = date_index(prices, 'XRP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter date only 01-01-2016 to 31-12-2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_date(df, after, before):\n",
    "    return df.ix[after:before]\n",
    "\n",
    "pricesLTC = filter_date(pricesLTC, '2016-01-01','2017-12-31')\n",
    "pricesETH = filter_date(pricesETH, '2016-01-01','2017-12-31')\n",
    "pricesXRP = filter_date(pricesXRP, '2016-01-01','2017-12-31')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate extra variables and add as new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def operations(df):\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    df['prices'] = df[['open', 'close','high','low']].mean(axis=1)\n",
    "    df['delta_day'] = df['high'] - df['low']\n",
    "    df['fluctuation'] = 0\n",
    "    df['fluctuation'] = df.delta_day.diff().fillna(0)\n",
    "    df['norm_fluctuation'] = 0\n",
    "    df['norm_fluctuation'] = scaler.fit_transform(df[['fluctuation']])\n",
    "    df['pct_change']= df['prices'].pct_change()\n",
    "    df['log_pct_change'] = np.log(df['prices'].astype('float64')/df['prices'].astype('float64').shift(1))\n",
    "    #df['norm_compound'] = 0\n",
    "    #df['norm_compound'] = scaler.fit_transform(df[['compound']])\n",
    "    \n",
    "operations(pricesLTC)\n",
    "operations(pricesETH)\n",
    "operations(pricesXRP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save price data to pickle for merging with sentiment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pricesLTC.to_pickle('pricesLTC2016-17.pkl')\n",
    "pricesXRP.to_pickle('pricesXRP2016-17.pkl')\n",
    "pricesETH.to_pickle('pricesETH2016-17.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 3. Merging, corrolation and visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Import comment data and price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ltc = pd.read_pickle('Litecoin2016-17.pkl')\n",
    "eth = pd.read_pickle('Ethereum2016-17.pkl')\n",
    "xrp = pd.read_pickle('Ripple2016-17.pkl')\n",
    "\n",
    "\n",
    "priceltc = pd.read_pickle('pricesLTC2016-17.pkl')\n",
    "pricexrp = pd.read_pickle('pricesXRP2016-17.pkl')\n",
    "priceeth = pd.read_pickle('pricesETH2016-17.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Merge datasets on index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def merge(leftdf, rightdf):\n",
    "    return pd.merge(leftdf,rightdf, how='inner', left_index=True, right_index=True)\n",
    "    \n",
    "\n",
    "mergeltc = merge(priceltc, ltc)\n",
    "mergeeth = merge(priceeth, eth)\n",
    "mergexrp = merge(pricexrp, xrp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output the corrolation matrix between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corr_matrix(df):\n",
    "    df = df[['prices','volume','delta_day', 'compound', 'numcomments']]\n",
    "    return df.corr(method='pearson')\n",
    "    \n",
    "corr_matrix(eth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display top corrolations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def correlation_top(df):\n",
    "    corre = df.corr(method='pearson')\n",
    "    c1 = corre.abs().unstack()\n",
    "    results = c1.sort_values(ascending = False)\n",
    "    print(results[0:150])\n",
    "    \n",
    "correlation_top(xrp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Set size of plot to fixed size 15x9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 15\n",
    "fig_size[1] = 9\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. LSTM prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Load the datasets for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ltc = pd.read_pickle('mergeLTC2016-17.pkl')\n",
    "eth = pd.read_pickle('mergeETH2016-17.pkl')\n",
    "xrp = pd.read_pickle('mergeXRP2016-17.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Steps for choosing the analysis:\n",
    "<br>\n",
    "- Copy data of cryptocurrency to temporary df\n",
    "- Specify the input variables as string in function call\n",
    "<br>\n",
    "eg: prices = 1 lag day prices, compound = 1 lag day compound\n",
    "<br>\n",
    "All variables:\n",
    "df = choose_crypto(ltc ,'prices','compound', 'numcomments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def choose_crypto(crypto, *vars):\n",
    "    df = crypto\n",
    "    if len(vars) == 4:\n",
    "        df = df[[vars[0],vars[1],vars[2],vars[3]]]\n",
    "    elif len(vars) == 3:\n",
    "        df = df[[vars[0],vars[1],vars[2]]]\n",
    "    elif len(vars) == 2:\n",
    "        df = df[[vars[0],vars[1]]]\n",
    "    else:\n",
    "        df = df[[vars[0]]]\n",
    "    return df\n",
    "\n",
    "df = choose_crypto(ltc, 'prices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Load dataset as values\n",
    "- Ensure all data is float\n",
    "- Normalize features using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = df.values\n",
    "values = values.astype('float32')\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Specify the number of lag days and features above function\n",
    "<br>\n",
    "INCORRECT n_features AND n_days WILL RESULT IN ERROR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(729, 2)\n",
      "   var1(t-1)   var1(t)\n",
      "1   0.001349  0.001385\n",
      "2   0.001385  0.001314\n",
      "3   0.001314  0.001335\n",
      "4   0.001335  0.001299\n",
      "5   0.001299  0.001236 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_days = 1\n",
    "n_features = 1\n",
    "\n",
    "# convert series to supervised learning by shifting t-1, t-2, t-3 depending on lag of days\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    #input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    #forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    \n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "\n",
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_days,1)\n",
    "\n",
    "print(reframed.shape)\n",
    "print(reframed.head(), '\\n')\n",
    "\n",
    "if df.shape[1] != n_features:\n",
    "    print('ERROR: n_features must match input variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Split into train and test sets\n",
    "<br>\n",
    "- Reshape input to be 3D [samples, timesteps, features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(583, 1) 583 (583,)\n",
      "(583, 1, 1) (583,) (146, 1, 1) (146,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gerlofbremmer/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:3: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n",
      "  app.launch_new_instance()\n",
      "/Users/gerlofbremmer/anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:4: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "values = reframed.values\n",
    "n_train_days = 0.8*len(reframed)\n",
    "train = values[:n_train_days, :]\n",
    "test = values[n_train_days:, :]\n",
    "\n",
    "#split into input and outputs\n",
    "n_obs = n_days * n_features\n",
    "train_X, train_y = train[:, :n_obs], train[:, -n_features]\n",
    "test_X, test_y = test[:, :n_obs], test[:, -n_features]\n",
    "print(train_X.shape, len(train_X), train_y.shape)\n",
    "\n",
    "train_X = train_X.reshape((train_X.shape[0], n_days, n_features))\n",
    "test_X = test_X.reshape((test_X.shape[0], n_days, n_features))\n",
    "print(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n",
    "\n",
    "if df.shape[1] != n_features:\n",
    "    print('ERROR: n_features must match input variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Compile the LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 20)                1760      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 1,781\n",
      "Trainable params: 1,781\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(20, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Fit the model and plot the losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 583 samples, validate on 146 samples\n",
      "Epoch 1/130\n",
      "583/583 [==============================] - 0s 613us/step - loss: 0.0181 - val_loss: 0.2596\n",
      "Epoch 2/130\n",
      "583/583 [==============================] - 0s 159us/step - loss: 0.0178 - val_loss: 0.2565\n",
      "Epoch 3/130\n",
      "583/583 [==============================] - 0s 159us/step - loss: 0.0180 - val_loss: 0.2551\n",
      "Epoch 4/130\n",
      "583/583 [==============================] - 0s 155us/step - loss: 0.0172 - val_loss: 0.2504\n",
      "Epoch 5/130\n",
      "583/583 [==============================] - 0s 158us/step - loss: 0.0181 - val_loss: 0.2495\n",
      "Epoch 6/130\n",
      "583/583 [==============================] - 0s 154us/step - loss: 0.0169 - val_loss: 0.2450\n",
      "Epoch 7/130\n",
      "583/583 [==============================] - 0s 155us/step - loss: 0.0178 - val_loss: 0.2440\n",
      "Epoch 8/130\n",
      "583/583 [==============================] - 0s 156us/step - loss: 0.0166 - val_loss: 0.2398\n",
      "Epoch 9/130\n",
      "583/583 [==============================] - 0s 158us/step - loss: 0.0175 - val_loss: 0.2387\n",
      "Epoch 10/130\n",
      "583/583 [==============================] - 0s 158us/step - loss: 0.0163 - val_loss: 0.2345\n",
      "Epoch 11/130\n",
      "583/583 [==============================] - 0s 157us/step - loss: 0.0171 - val_loss: 0.2334\n",
      "Epoch 12/130\n",
      "583/583 [==============================] - 0s 156us/step - loss: 0.0159 - val_loss: 0.2290\n",
      "Epoch 13/130\n",
      "583/583 [==============================] - 0s 159us/step - loss: 0.0168 - val_loss: 0.2278\n",
      "Epoch 14/130\n",
      "583/583 [==============================] - 0s 165us/step - loss: 0.0155 - val_loss: 0.2233\n",
      "Epoch 15/130\n",
      "583/583 [==============================] - 0s 186us/step - loss: 0.0165 - val_loss: 0.2221\n",
      "Epoch 16/130\n",
      "583/583 [==============================] - 0s 175us/step - loss: 0.0151 - val_loss: 0.2173\n",
      "Epoch 17/130\n",
      "583/583 [==============================] - 0s 159us/step - loss: 0.0162 - val_loss: 0.2159\n",
      "Epoch 18/130\n",
      "583/583 [==============================] - 0s 159us/step - loss: 0.0148 - val_loss: 0.2113\n",
      "Epoch 19/130\n",
      "583/583 [==============================] - 0s 165us/step - loss: 0.0157 - val_loss: 0.2100\n",
      "Epoch 20/130\n",
      "583/583 [==============================] - 0s 169us/step - loss: 0.0143 - val_loss: 0.2047\n",
      "Epoch 21/130\n",
      "583/583 [==============================] - 0s 164us/step - loss: 0.0154 - val_loss: 0.2033\n",
      "Epoch 22/130\n",
      "583/583 [==============================] - 0s 181us/step - loss: 0.0138 - val_loss: 0.1978\n",
      "Epoch 23/130\n",
      "583/583 [==============================] - 0s 177us/step - loss: 0.0150 - val_loss: 0.1965\n",
      "Epoch 24/130\n",
      "583/583 [==============================] - 0s 161us/step - loss: 0.0134 - val_loss: 0.1903\n",
      "Epoch 25/130\n",
      "583/583 [==============================] - 0s 180us/step - loss: 0.0147 - val_loss: 0.1887\n",
      "Epoch 26/130\n",
      "583/583 [==============================] - 0s 187us/step - loss: 0.0129 - val_loss: 0.1826\n",
      "Epoch 27/130\n",
      "583/583 [==============================] - 0s 171us/step - loss: 0.0143 - val_loss: 0.1807\n",
      "Epoch 28/130\n",
      "583/583 [==============================] - 0s 172us/step - loss: 0.0124 - val_loss: 0.1743\n",
      "Epoch 29/130\n",
      "583/583 [==============================] - 0s 160us/step - loss: 0.0138 - val_loss: 0.1724\n",
      "Epoch 30/130\n",
      "583/583 [==============================] - 0s 171us/step - loss: 0.0118 - val_loss: 0.1656\n",
      "Epoch 31/130\n",
      "583/583 [==============================] - 0s 190us/step - loss: 0.0132 - val_loss: 0.1638\n",
      "Epoch 32/130\n",
      "583/583 [==============================] - 0s 166us/step - loss: 0.0112 - val_loss: 0.1558\n",
      "Epoch 33/130\n",
      "583/583 [==============================] - 0s 173us/step - loss: 0.0128 - val_loss: 0.1537\n",
      "Epoch 34/130\n",
      "583/583 [==============================] - 0s 177us/step - loss: 0.0106 - val_loss: 0.1460\n",
      "Epoch 35/130\n",
      "583/583 [==============================] - 0s 180us/step - loss: 0.0121 - val_loss: 0.1439\n",
      "Epoch 36/130\n",
      "583/583 [==============================] - 0s 172us/step - loss: 0.0099 - val_loss: 0.1346\n",
      "Epoch 37/130\n",
      "583/583 [==============================] - 0s 198us/step - loss: 0.0119 - val_loss: 0.1317\n",
      "Epoch 38/130\n",
      "583/583 [==============================] - 0s 198us/step - loss: 0.0093 - val_loss: 0.1239\n",
      "Epoch 39/130\n",
      "583/583 [==============================] - 0s 174us/step - loss: 0.0108 - val_loss: 0.1212\n",
      "Epoch 40/130\n",
      "583/583 [==============================] - 0s 167us/step - loss: 0.0085 - val_loss: 0.1108\n",
      "Epoch 41/130\n",
      "583/583 [==============================] - 0s 173us/step - loss: 0.0106 - val_loss: 0.1072\n",
      "Epoch 42/130\n",
      "583/583 [==============================] - 0s 195us/step - loss: 0.0078 - val_loss: 0.0989\n",
      "Epoch 43/130\n",
      "583/583 [==============================] - 0s 188us/step - loss: 0.0091 - val_loss: 0.0958\n",
      "Epoch 44/130\n",
      "583/583 [==============================] - 0s 180us/step - loss: 0.0069 - val_loss: 0.0864\n",
      "Epoch 45/130\n",
      "583/583 [==============================] - 0s 199us/step - loss: 0.0074 - val_loss: 0.0807\n",
      "Epoch 46/130\n",
      "583/583 [==============================] - 0s 177us/step - loss: 0.0060 - val_loss: 0.0707\n",
      "Epoch 47/130\n",
      "583/583 [==============================] - 0s 163us/step - loss: 0.0070 - val_loss: 0.0667\n",
      "Epoch 48/130\n",
      "583/583 [==============================] - 0s 181us/step - loss: 0.0053 - val_loss: 0.0537\n",
      "Epoch 49/130\n",
      "583/583 [==============================] - 0s 203us/step - loss: 0.0065 - val_loss: 0.0516\n",
      "Epoch 50/130\n",
      "583/583 [==============================] - 0s 203us/step - loss: 0.0045 - val_loss: 0.0388\n",
      "Epoch 51/130\n",
      "583/583 [==============================] - 0s 172us/step - loss: 0.0049 - val_loss: 0.0346\n",
      "Epoch 52/130\n",
      "583/583 [==============================] - 0s 167us/step - loss: 0.0032 - val_loss: 0.0268\n",
      "Epoch 53/130\n",
      "583/583 [==============================] - 0s 170us/step - loss: 0.0034 - val_loss: 0.0238\n",
      "Epoch 54/130\n",
      "583/583 [==============================] - 0s 162us/step - loss: 0.0023 - val_loss: 0.0209\n",
      "Epoch 55/130\n",
      "583/583 [==============================] - 0s 181us/step - loss: 0.0019 - val_loss: 0.0213\n",
      "Epoch 56/130\n",
      "583/583 [==============================] - 0s 169us/step - loss: 0.0023 - val_loss: 0.0240\n",
      "Epoch 57/130\n",
      "583/583 [==============================] - 0s 161us/step - loss: 0.0015 - val_loss: 0.0267\n",
      "Epoch 58/130\n",
      "583/583 [==============================] - 0s 159us/step - loss: 0.0024 - val_loss: 0.0297\n",
      "Epoch 59/130\n",
      "583/583 [==============================] - 0s 156us/step - loss: 0.0015 - val_loss: 0.0270\n",
      "Epoch 60/130\n",
      "583/583 [==============================] - 0s 188us/step - loss: 0.0012 - val_loss: 0.0247\n",
      "Epoch 61/130\n",
      "583/583 [==============================] - 0s 164us/step - loss: 0.0013 - val_loss: 0.0246\n",
      "Epoch 62/130\n",
      "583/583 [==============================] - 0s 153us/step - loss: 0.0011 - val_loss: 0.0250\n",
      "Epoch 63/130\n",
      "583/583 [==============================] - 0s 156us/step - loss: 0.0013 - val_loss: 0.0255\n",
      "Epoch 64/130\n",
      "583/583 [==============================] - 0s 168us/step - loss: 0.0012 - val_loss: 0.0263\n",
      "Epoch 65/130\n",
      "583/583 [==============================] - 0s 164us/step - loss: 0.0013 - val_loss: 0.0251\n",
      "Epoch 66/130\n",
      "583/583 [==============================] - 0s 180us/step - loss: 0.0013 - val_loss: 0.0248\n",
      "Epoch 67/130\n",
      "583/583 [==============================] - 0s 187us/step - loss: 0.0013 - val_loss: 0.0246\n",
      "Epoch 68/130\n",
      "583/583 [==============================] - 0s 164us/step - loss: 0.0013 - val_loss: 0.0246\n",
      "Epoch 69/130\n",
      "583/583 [==============================] - 0s 178us/step - loss: 0.0013 - val_loss: 0.0246\n",
      "Epoch 70/130\n",
      "583/583 [==============================] - 0s 175us/step - loss: 0.0013 - val_loss: 0.0248\n",
      "Epoch 71/130\n",
      "583/583 [==============================] - 0s 179us/step - loss: 0.0013 - val_loss: 0.0247\n",
      "Epoch 72/130\n",
      "583/583 [==============================] - 0s 165us/step - loss: 0.0013 - val_loss: 0.0248\n",
      "Epoch 73/130\n",
      "583/583 [==============================] - 0s 165us/step - loss: 0.0013 - val_loss: 0.0247\n",
      "Epoch 74/130\n",
      "583/583 [==============================] - 0s 168us/step - loss: 0.0013 - val_loss: 0.0248\n",
      "Epoch 75/130\n",
      "583/583 [==============================] - 0s 162us/step - loss: 0.0013 - val_loss: 0.0247\n",
      "Epoch 76/130\n",
      "583/583 [==============================] - 0s 163us/step - loss: 0.0013 - val_loss: 0.0248\n",
      "Epoch 77/130\n",
      "583/583 [==============================] - 0s 159us/step - loss: 0.0013 - val_loss: 0.0247\n",
      "Epoch 78/130\n",
      "583/583 [==============================] - 0s 158us/step - loss: 0.0013 - val_loss: 0.0247\n",
      "Epoch 79/130\n",
      "583/583 [==============================] - 0s 156us/step - loss: 0.0013 - val_loss: 0.0247\n",
      "Epoch 80/130\n",
      "583/583 [==============================] - 0s 164us/step - loss: 0.0013 - val_loss: 0.0247\n",
      "Epoch 81/130\n",
      "583/583 [==============================] - 0s 160us/step - loss: 0.0013 - val_loss: 0.0249\n",
      "Epoch 82/130\n",
      "583/583 [==============================] - 0s 160us/step - loss: 0.0016 - val_loss: 0.0247\n",
      "Epoch 83/130\n",
      "583/583 [==============================] - 0s 158us/step - loss: 0.0020 - val_loss: 0.0246\n",
      "Epoch 84/130\n",
      "583/583 [==============================] - 0s 157us/step - loss: 0.0015 - val_loss: 0.0258\n",
      "Epoch 85/130\n",
      "583/583 [==============================] - 0s 155us/step - loss: 0.0014 - val_loss: 0.0262\n",
      "Epoch 86/130\n",
      "583/583 [==============================] - 0s 155us/step - loss: 0.0017 - val_loss: 0.0265\n",
      "Epoch 87/130\n",
      "583/583 [==============================] - 0s 153us/step - loss: 0.0014 - val_loss: 0.0255\n",
      "Epoch 88/130\n",
      "583/583 [==============================] - 0s 157us/step - loss: 0.0016 - val_loss: 0.0250\n",
      "Epoch 89/130\n",
      "583/583 [==============================] - 0s 156us/step - loss: 0.0015 - val_loss: 0.0247\n",
      "Epoch 90/130\n",
      "583/583 [==============================] - 0s 160us/step - loss: 0.0016 - val_loss: 0.0246\n",
      "Epoch 91/130\n",
      "583/583 [==============================] - 0s 173us/step - loss: 0.0015 - val_loss: 0.0245\n",
      "Epoch 92/130\n",
      "583/583 [==============================] - 0s 186us/step - loss: 0.0015 - val_loss: 0.0245\n",
      "Epoch 93/130\n",
      "583/583 [==============================] - 0s 178us/step - loss: 0.0015 - val_loss: 0.0246\n",
      "Epoch 94/130\n",
      "583/583 [==============================] - 0s 176us/step - loss: 0.0016 - val_loss: 0.0247\n",
      "Epoch 95/130\n",
      "583/583 [==============================] - 0s 177us/step - loss: 0.0015 - val_loss: 0.0246\n",
      "Epoch 96/130\n",
      "583/583 [==============================] - 0s 174us/step - loss: 0.0015 - val_loss: 0.0246\n",
      "Epoch 97/130\n",
      "583/583 [==============================] - 0s 177us/step - loss: 0.0015 - val_loss: 0.0245\n",
      "Epoch 98/130\n",
      "583/583 [==============================] - 0s 176us/step - loss: 0.0015 - val_loss: 0.0245\n",
      "Epoch 99/130\n",
      "583/583 [==============================] - 0s 212us/step - loss: 0.0015 - val_loss: 0.0244\n",
      "Epoch 100/130\n",
      "583/583 [==============================] - 0s 203us/step - loss: 0.0015 - val_loss: 0.0244\n",
      "Epoch 101/130\n",
      "583/583 [==============================] - 0s 191us/step - loss: 0.0015 - val_loss: 0.0244\n",
      "Epoch 102/130\n",
      "583/583 [==============================] - 0s 184us/step - loss: 0.0015 - val_loss: 0.0244\n",
      "Epoch 103/130\n",
      "583/583 [==============================] - 0s 173us/step - loss: 0.0015 - val_loss: 0.0244\n",
      "Epoch 104/130\n",
      "583/583 [==============================] - 0s 182us/step - loss: 0.0015 - val_loss: 0.0244\n",
      "Epoch 105/130\n",
      "583/583 [==============================] - 0s 183us/step - loss: 0.0015 - val_loss: 0.0244\n",
      "Epoch 106/130\n",
      "583/583 [==============================] - 0s 197us/step - loss: 0.0015 - val_loss: 0.0243\n",
      "Epoch 107/130\n",
      "583/583 [==============================] - 0s 195us/step - loss: 0.0015 - val_loss: 0.0243\n",
      "Epoch 108/130\n",
      "583/583 [==============================] - 0s 190us/step - loss: 0.0015 - val_loss: 0.0243\n",
      "Epoch 109/130\n",
      "583/583 [==============================] - 0s 196us/step - loss: 0.0015 - val_loss: 0.0243\n",
      "Epoch 110/130\n",
      "583/583 [==============================] - 0s 164us/step - loss: 0.0015 - val_loss: 0.0243\n",
      "Epoch 111/130\n",
      "583/583 [==============================] - 0s 171us/step - loss: 0.0015 - val_loss: 0.0242\n",
      "Epoch 112/130\n",
      "583/583 [==============================] - 0s 185us/step - loss: 0.0015 - val_loss: 0.0242\n",
      "Epoch 113/130\n",
      "583/583 [==============================] - 0s 174us/step - loss: 0.0015 - val_loss: 0.0242\n",
      "Epoch 114/130\n",
      "583/583 [==============================] - 0s 175us/step - loss: 0.0015 - val_loss: 0.0242\n",
      "Epoch 115/130\n",
      "583/583 [==============================] - 0s 170us/step - loss: 0.0015 - val_loss: 0.0243\n",
      "Epoch 116/130\n",
      "583/583 [==============================] - 0s 174us/step - loss: 0.0015 - val_loss: 0.0244\n",
      "Epoch 117/130\n",
      "583/583 [==============================] - 0s 176us/step - loss: 0.0015 - val_loss: 0.0244\n",
      "Epoch 118/130\n",
      "583/583 [==============================] - 0s 206us/step - loss: 0.0015 - val_loss: 0.0244\n",
      "Epoch 119/130\n",
      "583/583 [==============================] - 0s 198us/step - loss: 0.0015 - val_loss: 0.0243\n",
      "Epoch 120/130\n",
      "583/583 [==============================] - 0s 181us/step - loss: 0.0015 - val_loss: 0.0243\n",
      "Epoch 121/130\n",
      "583/583 [==============================] - 0s 173us/step - loss: 0.0015 - val_loss: 0.0242\n",
      "Epoch 122/130\n",
      "583/583 [==============================] - 0s 192us/step - loss: 0.0015 - val_loss: 0.0242\n",
      "Epoch 123/130\n",
      "583/583 [==============================] - 0s 209us/step - loss: 0.0015 - val_loss: 0.0241\n",
      "Epoch 124/130\n",
      "583/583 [==============================] - 0s 212us/step - loss: 0.0015 - val_loss: 0.0241\n",
      "Epoch 125/130\n",
      "583/583 [==============================] - 0s 200us/step - loss: 0.0015 - val_loss: 0.0241\n",
      "Epoch 126/130\n",
      "583/583 [==============================] - 0s 205us/step - loss: 0.0015 - val_loss: 0.0240\n",
      "Epoch 127/130\n",
      "583/583 [==============================] - 0s 189us/step - loss: 0.0015 - val_loss: 0.0240\n",
      "Epoch 128/130\n",
      "583/583 [==============================] - 0s 199us/step - loss: 0.0015 - val_loss: 0.0240\n",
      "Epoch 129/130\n",
      "583/583 [==============================] - 0s 188us/step - loss: 0.0015 - val_loss: 0.0240\n",
      "Epoch 130/130\n",
      "583/583 [==============================] - 0s 181us/step - loss: 0.0015 - val_loss: 0.0240\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niEiSUfGLiCQZFb+ISJJR8YuIJJn/AzD7FmFDkxiSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2055db38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = model.fit(train_X, train_y, epochs=130, batch_size=72, validation_data=(test_X, test_y), verbose=1, shuffle=False)\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Prediction of the model.\n",
    "<br>\n",
    "Following steps are taken:\n",
    "<br>\n",
    "- Prediction\n",
    "- Invert scaling for forecast\n",
    "- Invert scaling for actual\n",
    "- Calculate RMSE\n",
    "- Prediction on train\n",
    "- Invert scaling for train forecast\n",
    "- Invert scaling for train actual\n",
    "- Calculate train RMSE\n",
    "- Calculate MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE (Root Mean Squared Error): 1.037\n",
      "Test RMSE (Root Mean Squared Error): 18.527\n",
      "\n",
      "Train MAE (Mean Absolute Error): 0.601\n",
      "Test MAE (Mean Absolute Error): 8.452\n",
      "Mean Absolute Percentage Error =  5.52983433008\n"
     ]
    }
   ],
   "source": [
    "# make a prediction with model\n",
    "yhat = model.predict(test_X)\n",
    "test_X = test_X.reshape((test_X.shape[0], n_days*n_features))\n",
    "# invert scaling for forecast\n",
    "inv_yhat = concatenate((yhat, test_X[:, -(n_features-1):]), axis=1)\n",
    "inv_yhat = scaler.inverse_transform(inv_yhat)\n",
    "inv_yhat = inv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "test_y = test_y.reshape((len(test_y), 1))\n",
    "inv_y = concatenate((test_y, test_X[:, -(n_features-1):]), axis=1)\n",
    "inv_y = scaler.inverse_transform(inv_y)\n",
    "inv_y = inv_y[:,0]\n",
    "# calculate RMSE\n",
    "\n",
    "\n",
    "# make a prediction with model for train\n",
    "trainyhat = model.predict(train_X)\n",
    "train_X = train_X.reshape((train_X.shape[0], n_days*n_features))\n",
    "# invert scaling for forecast\n",
    "traininv_yhat = concatenate((trainyhat, train_X[:, -(n_features-1):]), axis=1)\n",
    "traininv_yhat = scaler.inverse_transform(traininv_yhat)\n",
    "traininv_yhat = traininv_yhat[:,0]\n",
    "# invert scaling for actual\n",
    "train_y = train_y.reshape((len(train_y), 1))\n",
    "traininv_y = concatenate((train_y, train_X[:, -(n_features-1):]), axis=1)\n",
    "traininv_y = scaler.inverse_transform(traininv_y)\n",
    "traininv_y = traininv_y[:,0]\n",
    "\n",
    "\n",
    "\n",
    "trainrmse = sqrt(mean_squared_error(traininv_y, traininv_yhat))\n",
    "testrmse = sqrt(mean_squared_error(inv_y, inv_yhat))\n",
    "\n",
    "print('Train RMSE (Root Mean Squared Error): %.3f' % trainrmse)\n",
    "print('Test RMSE (Root Mean Squared Error): %.3f' % testrmse)\n",
    "print('')\n",
    "\n",
    "testmae = mean_absolute_error(inv_y, inv_yhat)\n",
    "trainmae = mean_absolute_error(traininv_y, traininv_yhat)\n",
    "\n",
    "print('Train MAE (Mean Absolute Error): %.3f' % trainmae)\n",
    "print('Test MAE (Mean Absolute Error): %.3f' % testmae)\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "MAPE = mean_absolute_percentage_error(inv_y, inv_yhat )\n",
    "print(\"Mean Absolute Percentage Error = \", MAPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Plot predicted versus real values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntMbfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2137d748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(inv_yhat, label='predict')\n",
    "pyplot.plot(inv_y, label='actual', alpha=0.5)\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Create dataframe whith predicted, true and percentge difference between predicted and true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           pred        real  percentage change  pred_change  real_change\n",
      "0     45.689869   47.422501          -3.653607     0.000000     0.000000\n",
      "1     47.542709   48.497498          -1.968737     1.852840     1.074997\n",
      "2     48.663834   47.389999           2.687981     1.121124    -1.107498\n",
      "3     47.508835   46.807499           1.498341    -1.154999    -0.582500\n",
      "4     46.902039   46.847496           0.116426    -0.606796     0.039997\n",
      "5     46.943691   46.195000           1.620720     0.041653    -0.652496\n",
      "6     46.264500   45.892506           0.810577    -0.679192    -0.302494\n",
      "7     45.949841   44.312496           3.694995    -0.314659    -1.580009\n",
      "8     44.308353   43.787498           1.189506    -1.641487    -0.524998\n",
      "9     43.763702   44.174999          -0.931062    -0.544651     0.387501\n",
      "10    44.165668   45.657501          -3.267443     0.401966     1.482502\n",
      "11    45.705471   45.847496          -0.309777     1.539803     0.189995\n",
      "12    45.903034   45.977505          -0.161972     0.197563     0.130009\n",
      "13    46.038250   47.125000          -2.306101     0.135216     1.147495\n",
      "14    47.232731   47.002502           0.489822     1.194481    -0.122498\n",
      "15    47.105125   50.305000          -6.360948    -0.127605     3.302498\n",
      "16    50.552509   51.619999          -2.067977     3.447384     1.314999\n",
      "17    51.929401   50.830002           2.162895     1.376892    -0.789997\n",
      "18    51.101936   51.407501          -0.594397    -0.827465     0.577499\n",
      "19    51.706741   57.259998          -9.698318     0.604805     5.852497\n",
      "20    57.861721   62.127499          -6.866167     6.154980     4.867500\n",
      "21    63.016048   62.527496           0.781340     5.154327     0.399998\n",
      "22    63.441025   63.110001           0.524519     0.424976     0.582504\n",
      "23    64.060287   67.885002          -5.634109     0.619263     4.775002\n",
      "24    69.153397   78.580002         -11.996189     5.093109    10.695000\n",
      "25    80.667679   82.287498          -1.968488    11.514282     3.707497\n",
      "26    84.692825   78.065002           8.490133     4.025146    -4.222496\n",
      "27    80.109909   69.977501          14.479523    -4.582916    -8.087502\n",
      "28    71.394669   67.007500           6.547280    -8.715240    -2.970001\n",
      "29    68.215202   76.010002         -10.254966    -3.179466     9.002502\n",
      "..          ...         ...                ...          ...          ...\n",
      "116   96.183411  100.334999          -4.137727     6.841980     7.547501\n",
      "117  104.523544  101.485001           2.994081     8.340134     1.150002\n",
      "118  105.800087  101.602509           4.131372     1.276543     0.117508\n",
      "119  105.930611  103.205002           2.640966     0.130524     1.602493\n",
      "120  107.712128  101.239998           6.392858     1.781517    -1.965004\n",
      "121  105.528000   97.970001           7.714605    -2.184128    -3.269997\n",
      "122  101.903099  112.912498          -9.750381    -3.624901    14.942497\n",
      "123  118.565170  145.214996         -18.351980    16.662071    32.302498\n",
      "124  155.382599  147.520004           5.329850    36.817429     2.305008\n",
      "125  158.047989  183.772491         -13.998015     2.665390    36.252487\n",
      "126  200.550262  270.005005         -25.723501    42.502274    86.232513\n",
      "127  304.877380  311.809998          -2.223347   104.327118    41.804993\n",
      "128  356.345154  289.377502          23.141968    51.467773   -22.432495\n",
      "129  328.691986  287.095001          14.488927   -27.653168    -2.282501\n",
      "130  325.882202  299.584991           8.777880    -2.809784    12.489990\n",
      "131  341.267609  313.717499           8.781821    15.385406    14.132507\n",
      "132  358.698944  335.054993           7.056738    17.431335    21.337494\n",
      "133  385.036621  355.087494           8.434295    26.337677    20.032501\n",
      "134  409.753937  330.612488          23.937828    24.717316   -24.475006\n",
      "135  379.552734  317.847504          19.413469   -30.201202   -12.764984\n",
      "136  363.795776  272.992493          33.262192   -15.756958   -44.855011\n",
      "137  308.544434  282.962494           9.040752   -55.251343     9.970001\n",
      "138  320.797394  277.260010          15.702727    12.252960    -5.702484\n",
      "139  313.786469  276.809998          13.358068    -7.010925    -0.450012\n",
      "140  313.233459  280.345001          11.731423    -0.553009     3.535004\n",
      "141  317.578491  278.529999          14.019492     4.345032    -1.815002\n",
      "142  315.347290  256.654999          22.868166    -2.231201   -21.875000\n",
      "143  288.520538  248.054993          16.313135   -26.826752    -8.600006\n",
      "144  278.013275  229.419998          21.180925   -10.507263   -18.634995\n",
      "145  255.342239  225.167496          13.401021   -22.671036    -4.252502\n",
      "\n",
      "[146 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "d = {'real':inv_y, 'pred':inv_yhat}\n",
    "pred = DataFrame(data=d)\n",
    "pred['percentage change'] = (((pred['pred'] - pred['real'])/(pred['real']))*100)\n",
    "pred['pred_change'] = pred.pred.diff().fillna(0)\n",
    "pred['real_change'] = pred.real.diff().fillna(0)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculate acurracy:\n",
    "\n",
    "\\begin{equation*}\n",
    "Accuracy = \\frac{TP+TN}{(TP + TN + TP + TN)}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of correct direction prediction (up/down):  61.64383561643836\n"
     ]
    }
   ],
   "source": [
    "def accuracy(df):\n",
    "    count = 0\n",
    "    for index, row in pred.iterrows():\n",
    "        if row['pred_change'] < 0 and row['real_change'] < 0 or row['pred_change'] > 0 and row['real_change'] > 0:\n",
    "            count += 1\n",
    "    return count/len(df)*100\n",
    "\n",
    "print('Accuracy of correct direction prediction (up/down): ', accuracy(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of incorrect prediction how far off (up/down):  4.81136349327\n"
     ]
    }
   ],
   "source": [
    "def variance(df):\n",
    "    count = 0\n",
    "    percentage = 0\n",
    "    for index, row in pred.iterrows():\n",
    "        if row['pred_change'] < 0 and row['real_change'] > 0 or row['pred_change'] > 0 and row['real_change'] < 0:\n",
    "            count += 1\n",
    "            percentage += abs(row['percentage change'])\n",
    "    return percentage/count\n",
    "\n",
    "print('Variance of incorrect prediction how far off (up/down): ', variance(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Calculate F1-score by creating classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score: 66.66666666666667\n"
     ]
    }
   ],
   "source": [
    "def f1_score(df):\n",
    "    TN = 0\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "    FP = 0\n",
    "    for index, row in pred.iterrows():\n",
    "            if row['pred_change'] < 0 and row['real_change'] < 0: \n",
    "                TN += 1\n",
    "            elif row['pred_change'] > 0 and row['real_change'] > 0:\n",
    "                TP += 1\n",
    "            elif row['pred_change'] < 0 and row['real_change'] > 0:\n",
    "                FN += 1\n",
    "            elif row['pred_change'] > 0 and row['real_change'] < 0:\n",
    "                FP += 1\n",
    "\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    f1 = 2*((recall*precision)/(recall + precision))\n",
    "    return f1*100\n",
    "\n",
    "print('The F1 score:',f1_score(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
